import logging

from dotenv import load_dotenv
from openai import OpenAI
from openai.types.chat.chat_completion import ChatCompletion

from context import ifac_2025_context


class GPTInterface:
    def __init__(
        self,
        logger: logging.Logger,
        token_path: str,
        max_tokens: int = 2000,
        temperature: float = 0.2,
    ):
        self.logger: logging.Logger = logger
        # max number of tokens that GPT will respond with, almost 1:1 with words to token
        self.max_tokens: int = max_tokens
        # creativity of ChatGPT
        self.temperature: float = temperature
        # loading in secret API token from your env file
        load_dotenv(token_path)
        # binding GPT client
        self.client: OpenAI = OpenAI()
        # schema text
        self.schemas: str = ""

    def init_context(self, schema_path: list[str], context_files: list[str]):
        for s in schema_path:
            # all robots must come with a schema
            self._set_schema(s)

        # context can be updated from context.py
        self.context: list = ifac_2025_context(self.schemas)

        # this could be empty
        if context_files is not None:
            if len(context_files) > 0:
                self.context += self._add_additional_context_files(context_files)
        # input template file provided when wanting spin verification
        self.promela_template: str|None = None

    def init_xml_mp_context(self, schema_path: str, farm_layout_path: str):
        self._set_schema(schema_path)
        self._set_farm_layout(farm_layout_path)

        # default context
        self.context: list = [
            {
                "role": "system",
                "content": "You are a mission planner that generates navigational XML mission plans based on robotic task representation. \
                            When asked to generate a mission, create an XML file conformant to the known schema and \
                            use the GeoJSON file to provide references in the mission plan for things such as GPS location, tree type, etc. \
                            In order to accomplish most actions, you must first drive to the location. \
                            Therefore, tasks should almost always require driving to a tree and then doing an action unless doing multiple actions at the same tree. \
                            Place the original question in the TaskDescription element of the CompositeTaskInformation element for logging. \
                            The task names shall always follow the format of, \"Task#\", where # represents the number in the sequence of the tasks to be performed.",
            },
            # context
            {
                "role": "user",
                "content": "This is the schema for which you must generate mission plan XML documents. \
                            The mission must be syntactically correct and validate using an XML linter: "
                + self.schema,
            },
            {
                "role": "user",
                "content": "This is the GeoJSON for which you must generate mission plan XML documents. This is our orchard: "
                + self.farm_layout,
            },
        ]

        self.initial_context_length = len(self.context)

    def init_promela_context(self, schema_path: str, promela_template_path: str, farm_layout_path: str):
        # TODO: I think we need a list of task names or a way to format the task naming based on some kind of standard
        self._set_schema(schema_path)
        # NOTE: the farm layout will be relevant only if there is no task list, I believe.
        self._set_farm_layout(farm_layout_path)
        self._set_promela_template(promela_template_path)

        # default context
        self.context: list = [
            {
                "role": "system",
                "content": "You are a linear temporal logic generator that generates Spin compatible LTL missions based on mission input. \
                            The outputs should be able to be appended to a Promela system file to compile and formally verify in Spin. \
                            The task names in the LTL shall always follow the format of, \"Task#\", where # represents the number in the sequence of the tasks to be performed. ",
            },
            # context
            {
                "role": "user",
                "content": "Note that this is the XSD file that all missions will be generated by. \
                            When generating LTL equations, please map the types in this file to the types in the Promela template file."
                + self.schemas,
            },
            {
                "role": "user",
                "content": "This is the Promela template for object types available to generate an LTL. \
                            Use these objects types and only these object types when generating LTLs for Spin. \
                            Make the resultant LTL formulas as detailed as possible leveraging these object types that will be used in the Promela file."
                + self.promela_template,
            },
            {
                "role": "user",
                "content": "This is the GeoJSON for which you must generate mission plan XML documents. This is our orchard: "
                + self.farm_layout,
            },        
            # TODO: add context for examples of LTL generation to produce better results?
        ]

        self.initial_context_length = len(self.context)

    def add_context(self, user: str, assistant: str | None) -> None:
        # generate new GPT API dict string context
        new_user_context = {"role": "user", "content": user}
        new_assistant_context = {"role": "assistant", "content": assistant}
        # append to pre-existing context
        self.context.append(new_user_context)
        self.context.append(new_assistant_context)

    def reset_context(self):
        self.context = self.context[0 : self.initial_context_length]

    # TODO: should we expose OpenAI object or string response?
    def ask_gpt(self, prompt: str, add_context: bool = False) -> str | None:
        message: list = self.context.copy()
        message.append({"role": "user", "content": prompt})

        completion: ChatCompletion = self.client.chat.completions.create(
            model="gpt-4o",
            # model="o1-mini",
            messages=message,
            max_completion_tokens=self.max_tokens,
            # NOTE: this has been deprecated in o1-mini
            temperature=self.temperature,
        )

        response: str | None = completion.choices[0].message.content

        if add_context:
            self.add_context(prompt, response)

        return response

    def _set_schema(self, schema_path: str) -> None:
        # Read XSD from 1872.1-2024
        with open(schema_path, "r") as file:
            self.schemas += file.read()

        self.schemas += "\nThis schema is located at path: " + schema_path

        # deliniate for chatgpt
        self.schemas += "\nnext schema: "

    def _add_additional_context_files(self, context_files: list[str]) -> list[dict]:
        context_list: list[dict] = []
        for c in context_files:
            with open(c, "r") as file:
                extra = file.read()
            context_list.append(
                {
                    "role": "user",
                    "content": "Use this additional file to provide context when generating XML mission plans. \
                            The content within should be self explanatory: "
                    + extra,
                }
            )

        return context_list
    
    def _set_farm_layout(self, farm_layout_path: str) -> None:
        with open(farm_layout_path, "r") as file:
            self.farm_layout = file.read()

    def _set_promela_template(self, promela_template_path: str) -> None:
        with open(promela_template_path, "r") as file:
            self.promela_template = file.read()

    def get_promela_template(self) -> str:
        return self.promela_template
